{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demographic Embeddings: Visualizing Canadian City Similarities Across Time\n",
    "\n",
    "## Background and Context\n",
    "\n",
    "Back in 2018, I explored how Canadian cities compare demographically using t-SNE to visualize high-dimensional Census data in two dimensions. At the time, t-SNE was cutting-edge for this type of analysis, but it had limitations - particularly for temporal analysis and preserving global structure.\n",
    "\n",
    "Since then, the field of dimensionality reduction has exploded with new methods that address many of t-SNE's shortcomings. This analysis serves three purposes: (1) testing our pycancensus package by replicating the original work, (2) updating results with 2021 Census data, and (3) exploring how modern embedding methods reveal different aspects of Canadian urban demographics.\n",
    "\n",
    "The key innovation here is visualizing both 2016 and 2021 data simultaneously to track how cities have changed demographically over time - something that was challenging with traditional t-SNE.\n",
    "\n",
    "**Research Question**: How do Canadian cities cluster demographically, and how have these relationships evolved between 2016 and 2021?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Import pycancensus\n",
    "import pycancensus as pc\n",
    "\n",
    "# Set up clean plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Libraries loaded successfully\")\n",
    "print(f\"API key status: {'Set' if pc.get_api_key() else 'Not set'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and import available embedding methods\n",
    "available_methods = {}\n",
    "\n",
    "# Test UMAP\n",
    "try:\n",
    "    import umap\n",
    "    available_methods['UMAP'] = umap\n",
    "    print(\"✓ UMAP available\")\n",
    "except ImportError:\n",
    "    print(\"✗ UMAP not available\")\n",
    "\n",
    "# Test TriMAP  \n",
    "try:\n",
    "    import trimap\n",
    "    available_methods['TriMAP'] = trimap\n",
    "    print(\"✓ TriMAP available\")\n",
    "except ImportError:\n",
    "    print(\"✗ TriMAP not available\")\n",
    "\n",
    "# Test PaCMAP\n",
    "try:\n",
    "    import pacmap\n",
    "    available_methods['PaCMAP'] = pacmap\n",
    "    print(\"✓ PaCMAP available\")\n",
    "except ImportError:\n",
    "    print(\"✗ PaCMAP not available\")\n",
    "\n",
    "# Test PHATE\n",
    "try:\n",
    "    import phate\n",
    "    available_methods['PHATE'] = phate\n",
    "    print(\"✓ PHATE available\")\n",
    "except ImportError:\n",
    "    print(\"✗ PHATE not available\")\n",
    "\n",
    "# sklearn t-SNE is always available\n",
    "available_methods['t-SNE'] = TSNE\n",
    "print(\"✓ sklearn t-SNE available\")\n",
    "\n",
    "print(f\"\\nTotal available methods: {len(available_methods)}\")\n",
    "print(f\"Methods: {list(available_methods.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection and Harmonization\n",
    "\n",
    "We'll focus on visible minority demographics as in the original post, but extend to both 2016 and 2021 Census data. The challenge is harmonizing variable definitions across census years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define visible minority vectors for both census years\n",
    "# 2016 Census visible minority vectors\n",
    "visible_minority_2016 = {\n",
    "    'v_CA16_3954': 'Total_Population_VM',\n",
    "    'v_CA16_3957': 'South_Asian',\n",
    "    'v_CA16_3960': 'Chinese', \n",
    "    'v_CA16_3963': 'Black',\n",
    "    'v_CA16_3966': 'Filipino',\n",
    "    'v_CA16_3969': 'Latin_American',\n",
    "    'v_CA16_3972': 'Arab',\n",
    "    'v_CA16_3975': 'Southeast_Asian',\n",
    "    'v_CA16_3978': 'West_Asian',\n",
    "    'v_CA16_3981': 'Korean',\n",
    "    'v_CA16_3984': 'Japanese',\n",
    "    'v_CA16_3993': 'Not_Visible_Minority'\n",
    "}\n",
    "\n",
    "# 2021 Census visible minority vectors (harmonized mapping)\n",
    "visible_minority_2021 = {\n",
    "    'v_CA21_4872': 'Total_Population_VM',\n",
    "    'v_CA21_4875': 'South_Asian',\n",
    "    'v_CA21_4878': 'Chinese',\n",
    "    'v_CA21_4881': 'Black', \n",
    "    'v_CA21_4884': 'Filipino',\n",
    "    'v_CA21_4887': 'Arab',\n",
    "    'v_CA21_4890': 'Latin_American',\n",
    "    'v_CA21_4893': 'Southeast_Asian',\n",
    "    'v_CA21_4896': 'West_Asian',\n",
    "    'v_CA21_4899': 'Korean',\n",
    "    'v_CA21_4902': 'Japanese',\n",
    "    'v_CA21_4911': 'Not_Visible_Minority'\n",
    "}\n",
    "\n",
    "# Population vectors\n",
    "population_vectors = {\n",
    "    2016: 'v_CA16_1',\n",
    "    2021: 'v_CA21_1'\n",
    "}\n",
    "\n",
    "print(\"Variable mappings defined\")\n",
    "print(f\"2016 variables: {len(visible_minority_2016)}\")\n",
    "print(f\"2021 variables: {len(visible_minority_2021)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_census_data(year, vectors, population_vector, min_population=75000):\n",
    "    \"\"\"\n",
    "    Collect census data for Canadian cities above minimum population threshold\n",
    "    (Using same threshold as original 2018 analysis: ~75,000)\n",
    "    \"\"\"\n",
    "    print(f\"Collecting {year} Census data...\")\n",
    "    \n",
    "    # Get all Census Subdivisions (cities) across Canada\n",
    "    data = pc.get_census(\n",
    "        dataset=f'CA{str(year)[2:]}',\n",
    "        regions={'C': '01'},  # All of Canada\n",
    "        vectors=list(vectors.keys()) + [population_vector],\n",
    "        level='CSD',  # Census Subdivision (cities)\n",
    "        use_cache=True\n",
    "    )\n",
    "    \n",
    "    # Filter for minimum population (matching original analysis)\n",
    "    data = data[data['pop'] >= min_population].copy()\n",
    "    \n",
    "    # Calculate proportions for each visible minority group\n",
    "    for vector_code, group_name in vectors.items():\n",
    "        if vector_code in data.columns:\n",
    "            data[f'{group_name}_prop'] = data[vector_code] / data['pop']\n",
    "    \n",
    "    # Add year identifier\n",
    "    data['year'] = year\n",
    "    \n",
    "    print(f\"Collected data for {len(data)} cities with population >= {min_population:,}\")\n",
    "    return data\n",
    "\n",
    "# Collect data for both years\n",
    "print(\"Starting data collection...\\n\")\n",
    "data_2016 = collect_census_data(2016, visible_minority_2016, population_vectors[2016])\n",
    "data_2021 = collect_census_data(2021, visible_minority_2021, population_vectors[2021])\n",
    "\n",
    "print(f\"\\nData collection complete!\")\n",
    "print(f\"2016: {len(data_2016)} cities\")\n",
    "print(f\"2021: {len(data_2021)} cities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harmonize data across years and filter to cities present in both censuses\n",
    "def harmonize_data(data_2016, data_2021):\n",
    "    \"\"\"\n",
    "    Harmonize datasets and filter to cities present in both years\n",
    "    \"\"\"\n",
    "    # Get proportion columns\n",
    "    prop_cols = [col for col in data_2016.columns if col.endswith('_prop')]\n",
    "    \n",
    "    # Key columns to keep\n",
    "    keep_cols = ['GeoUID', 'Region Name', 'pop', 'year'] + prop_cols\n",
    "    \n",
    "    # Subset both datasets\n",
    "    df_2016 = data_2016[keep_cols].copy()\n",
    "    df_2021 = data_2021[keep_cols].copy()\n",
    "    \n",
    "    # Find cities present in both years (using GeoUID)\n",
    "    common_cities = set(df_2016['GeoUID']) & set(df_2021['GeoUID'])\n",
    "    \n",
    "    # Filter to common cities\n",
    "    df_2016 = df_2016[df_2016['GeoUID'].isin(common_cities)]\n",
    "    df_2021 = df_2021[df_2021['GeoUID'].isin(common_cities)]\n",
    "    \n",
    "    # Combine datasets\n",
    "    combined_data = pd.concat([df_2016, df_2021], ignore_index=True)\n",
    "    \n",
    "    print(f\"Harmonized data: {len(common_cities)} cities across both census years\")\n",
    "    print(f\"Combined dataset shape: {combined_data.shape}\")\n",
    "    \n",
    "    return combined_data, prop_cols\n",
    "\n",
    "combined_data, demographic_cols = harmonize_data(data_2016, data_2021)\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample of harmonized data:\")\n",
    "sample_cities = combined_data.groupby('GeoUID').first().nlargest(5, 'pop')\n",
    "print(sample_cities[['Region Name', 'pop'] + demographic_cols[:3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Following the original methodology, we'll standardize the demographic proportions and prepare the feature matrix for embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature matrix for embedding\n",
    "def prepare_features(data, demographic_cols):\n",
    "    \"\"\"\n",
    "    Prepare standardized feature matrix for embedding algorithms\n",
    "    \"\"\"\n",
    "    # Create feature matrix\n",
    "    X = data[demographic_cols].values\n",
    "    \n",
    "    # Remove any rows with missing values\n",
    "    valid_mask = ~np.isnan(X).any(axis=1)\n",
    "    X = X[valid_mask]\n",
    "    data_clean = data[valid_mask].copy()\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    print(f\"Feature matrix shape: {X_scaled.shape}\")\n",
    "    print(f\"Features: {len(demographic_cols)} demographic proportions\")\n",
    "    \n",
    "    return X_scaled, data_clean, scaler\n",
    "\n",
    "X_scaled, data_clean, scaler = prepare_features(combined_data, demographic_cols)\n",
    "\n",
    "# Show feature importance via PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "print(f\"\\nPCA Explained Variance Ratios (first 5 components):\")\n",
    "for i, ratio in enumerate(pca.explained_variance_ratio_[:5]):\n",
    "    print(f\"PC{i+1}: {ratio:.3f}\")\n",
    "print(f\"Cumulative variance explained (5 components): {pca.explained_variance_ratio_[:5].sum():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Methods Implementation\n",
    "\n",
    "Now we'll implement the available embedding methods to compare their strengths for temporal demographic analysis.\n",
    "\n",
    "### Evolution of Embedding Methods (2008-2025)\n",
    "\n",
    "| Year | Method | Key Innovation |\n",
    "|------|--------|----------------|\n",
    "| 2008 | t-SNE | Local structure preservation breakthrough |\n",
    "| 2018 | UMAP | Faster, manifold-aware global structure |\n",
    "| 2019 | TriMAP | Triplet loss for better global fidelity |\n",
    "| 2020 | PaCMAP | Balanced local/global structure |\n",
    "| 2019 | PHATE | Diffusion-based smooth trajectories |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding function implementations\n",
    "def run_tsne(X, random_state=42):\n",
    "    \"\"\"\n",
    "    Standard t-SNE embedding (sklearn implementation)\n",
    "    \"\"\"\n",
    "    print(\"Running t-SNE...\")\n",
    "    \n",
    "    tsne = TSNE(\n",
    "        n_components=2,\n",
    "        perplexity=30,\n",
    "        learning_rate='auto',\n",
    "        n_iter=1000,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    embedding = tsne.fit_transform(X)\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "def run_umap(X, random_state=42):\n",
    "    \"\"\"\n",
    "    UMAP embedding\n",
    "    \"\"\"\n",
    "    print(\"Running UMAP...\")\n",
    "    \n",
    "    umap_model = umap.UMAP(\n",
    "        n_neighbors=15,\n",
    "        min_dist=0.1,\n",
    "        n_components=2,\n",
    "        metric='euclidean',\n",
    "        random_state=random_state\n",
    "    )\n",
    "    embedding = umap_model.fit_transform(X)\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "def run_trimap(X, random_state=42):\n",
    "    \"\"\"\n",
    "    TriMAP embedding\n",
    "    \"\"\"\n",
    "    print(\"Running TriMAP...\")\n",
    "    \n",
    "    embedding = trimap.TRIMAP(\n",
    "        n_inliers=10,\n",
    "        n_outliers=5,\n",
    "        n_random=5,\n",
    "        distance='euclidean',\n",
    "        verbose=False\n",
    "    ).fit_transform(X)\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "def run_pacmap(X, random_state=42):\n",
    "    \"\"\"\n",
    "    PaCMAP embedding\n",
    "    \"\"\"\n",
    "    print(\"Running PaCMAP...\")\n",
    "    \n",
    "    # Handle the known broadcasting issue by using smaller dataset if needed\n",
    "    try:\n",
    "        embedding = pacmap.PaCMAP(\n",
    "            n_components=2,\n",
    "            n_neighbors=min(10, len(X)//4),  # Adaptive neighbors\n",
    "            MN_ratio=0.5,\n",
    "            FP_ratio=2.0,\n",
    "            distance='euclidean',\n",
    "            lr=1.0,\n",
    "            num_iters=450,\n",
    "            verbose=False,\n",
    "            random_state=random_state\n",
    "        ).fit_transform(X)\n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        print(f\"PaCMAP failed: {e}\")\n",
    "        print(\"Falling back to UMAP...\")\n",
    "        return run_umap(X, random_state)\n",
    "\n",
    "def run_phate(X, random_state=42):\n",
    "    \"\"\"\n",
    "    PHATE embedding (if available)\n",
    "    \"\"\"\n",
    "    print(\"Running PHATE...\")\n",
    "    \n",
    "    phate_op = phate.PHATE(\n",
    "        n_components=2,\n",
    "        knn=5,\n",
    "        decay=40,\n",
    "        n_landmark=min(2000, len(X)),\n",
    "        t='auto',\n",
    "        gamma=1,\n",
    "        n_pca=min(100, X.shape[1]),\n",
    "        mds_solver='sgd',\n",
    "        knn_dist='euclidean',\n",
    "        mds_dist='euclidean',\n",
    "        n_jobs=1,\n",
    "        random_state=random_state,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    embedding = phate_op.fit_transform(X)\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "print(\"Embedding functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all available embedding methods\n",
    "embeddings_dict = {}\n",
    "\n",
    "# Define method mappings\n",
    "method_functions = {\n",
    "    't-SNE': run_tsne,\n",
    "    'UMAP': run_umap,\n",
    "    'TriMAP': run_trimap,\n",
    "    'PaCMAP': run_pacmap,\n",
    "    'PHATE': run_phate\n",
    "}\n",
    "\n",
    "print(\"Running embedding methods...\\n\")\n",
    "\n",
    "for method_name in available_methods.keys():\n",
    "    if method_name in method_functions:\n",
    "        try:\n",
    "            embedding = method_functions[method_name](X_scaled)\n",
    "            embeddings_dict[method_name] = embedding\n",
    "            print(f\"✓ {method_name} completed: {embedding.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ {method_name} failed: {e}\")\n",
    "    else:\n",
    "        print(f\"⚠ {method_name} function not implemented\")\n",
    "\n",
    "print(f\"\\nSuccessfully completed {len(embeddings_dict)} embedding methods\")\n",
    "print(f\"Methods: {list(embeddings_dict.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Results Storage\n",
    "\n",
    "Now we'll organize all embedding results with the schema: city_id | year | method | x | y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results dataframe\n",
    "def create_results_dataframe(data_clean, embeddings_dict):\n",
    "    \"\"\"\n",
    "    Create comprehensive results with schema: city_id | year | method | x | y\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for method_name, coords in embeddings_dict.items():\n",
    "        for i, (_, row) in enumerate(data_clean.iterrows()):\n",
    "            results.append({\n",
    "                'city_id': row['GeoUID'],\n",
    "                'city_name': row['Region Name'],\n",
    "                'year': row['year'],\n",
    "                'population': row['pop'],\n",
    "                'method': method_name,\n",
    "                'x': coords[i, 0],\n",
    "                'y': coords[i, 1]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = create_results_dataframe(data_clean, embeddings_dict)\n",
    "\n",
    "print(f\"Results dataframe shape: {results_df.shape}\")\n",
    "print(f\"Methods: {results_df['method'].unique()}\")\n",
    "print(f\"Years: {results_df['year'].unique()}\")\n",
    "print(f\"Cities: {results_df['city_id'].nunique()}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample results:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization and Trajectory Analysis\n",
    "\n",
    "Now we'll create comprehensive visualizations comparing all methods, with particular attention to temporal trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify major cities for trajectory analysis\n",
    "major_cities_df = results_df.groupby('city_id').agg({\n",
    "    'city_name': 'first',\n",
    "    'population': 'mean'\n",
    "}).nlargest(12, 'population')\n",
    "\n",
    "major_cities = major_cities_df.index.tolist()\n",
    "\n",
    "print(\"Major cities for trajectory analysis:\")\n",
    "for i, (city_id, row) in enumerate(major_cities_df.iterrows()):\n",
    "    print(f\"{i+1:2d}. {row['city_name']:<25} (Pop: {row['population']:>8,.0f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization comparing all methods\n",
    "def plot_embedding_comparison(results_df, major_cities, figsize=(20, 15)):\n",
    "    \"\"\"\n",
    "    Create subplot comparison of all embedding methods\n",
    "    \"\"\"\n",
    "    methods = results_df['method'].unique()\n",
    "    n_methods = len(methods)\n",
    "    \n",
    "    # Create subplot grid\n",
    "    n_cols = 3 if n_methods > 2 else n_methods\n",
    "    n_rows = (n_methods + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    if n_rows == 1 and n_cols == 1:\n",
    "        axes = [axes]\n",
    "    elif n_rows == 1:\n",
    "        axes = axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    colors = {'2016': '#2E86AB', '2021': '#A23B72'}\n",
    "    \n",
    "    for i, method in enumerate(methods):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        method_data = results_df[results_df['method'] == method]\n",
    "        \n",
    "        # Plot points for each year\n",
    "        for year in [2016, 2021]:\n",
    "            year_data = method_data[method_data['year'] == year]\n",
    "            ax.scatter(year_data['x'], year_data['y'], \n",
    "                      c=colors[str(year)], alpha=0.7, s=50, \n",
    "                      label=f'{year}', edgecolors='white', linewidth=0.5)\n",
    "        \n",
    "        # Draw trajectories for major cities\n",
    "        for city_id in major_cities:\n",
    "            city_data = method_data[method_data['city_id'] == city_id].sort_values('year')\n",
    "            if len(city_data) == 2:  # Both years present\n",
    "                # Draw trajectory line\n",
    "                ax.plot(city_data['x'], city_data['y'], \n",
    "                       'k-', alpha=0.4, linewidth=1.5, zorder=1)\n",
    "                \n",
    "                # Add arrow to show direction  \n",
    "                ax.annotate('', xy=(city_data.iloc[1]['x'], city_data.iloc[1]['y']),\n",
    "                           xytext=(city_data.iloc[0]['x'], city_data.iloc[0]['y']),\n",
    "                           arrowprops=dict(arrowstyle='->', color='darkred', \n",
    "                                         alpha=0.7, lw=1.5, shrinkA=2, shrinkB=2))\n",
    "        \n",
    "        ax.set_title(f'{method}\\nDemographic Trajectories 2016→2021', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Dimension 1')\n",
    "        ax.set_ylabel('Dimension 2')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create the visualization\n",
    "plot_embedding_comparison(results_df, major_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed trajectory analysis\n",
    "def analyze_trajectories(results_df):\n",
    "    \"\"\"\n",
    "    Analyze city trajectories across embedding methods\n",
    "    \"\"\"\n",
    "    trajectory_stats = []\n",
    "    \n",
    "    for method in results_df['method'].unique():\n",
    "        method_data = results_df[results_df['method'] == method]\n",
    "        \n",
    "        # Calculate trajectory lengths for each city\n",
    "        for city_id in method_data['city_id'].unique():\n",
    "            city_data = method_data[method_data['city_id'] == city_id].sort_values('year')\n",
    "            \n",
    "            if len(city_data) == 2:\n",
    "                # Calculate trajectory length\n",
    "                dx = city_data.iloc[1]['x'] - city_data.iloc[0]['x']\n",
    "                dy = city_data.iloc[1]['y'] - city_data.iloc[0]['y']\n",
    "                trajectory_length = np.sqrt(dx**2 + dy**2)\n",
    "                \n",
    "                trajectory_stats.append({\n",
    "                    'method': method,\n",
    "                    'city_id': city_id,\n",
    "                    'city_name': city_data.iloc[0]['city_name'],\n",
    "                    'trajectory_length': trajectory_length,\n",
    "                    'dx': dx,\n",
    "                    'dy': dy,\n",
    "                    'population': city_data['population'].mean()\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(trajectory_stats)\n",
    "\n",
    "# Analyze trajectories\n",
    "trajectories = analyze_trajectories(results_df)\n",
    "\n",
    "# Summary statistics by method\n",
    "trajectory_summary = trajectories.groupby('method')['trajectory_length'].agg([\n",
    "    'mean', 'median', 'std', 'min', 'max'\n",
    "]).round(3)\n",
    "\n",
    "print(\"Trajectory Length Statistics by Method:\")\n",
    "print(trajectory_summary)\n",
    "\n",
    "# Cities with largest demographic changes\n",
    "print(\"\\nCities with Largest Demographic Changes (Average across methods):\")\n",
    "avg_trajectories = trajectories.groupby(['city_id', 'city_name'])['trajectory_length'].mean().sort_values(ascending=False)\n",
    "print(avg_trajectories.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create focused comparison of best methods\n",
    "def plot_best_methods(results_df, major_cities):\n",
    "    \"\"\"\n",
    "    Focus on the most promising methods for temporal analysis\n",
    "    \"\"\"\n",
    "    # Select best available methods\n",
    "    available_methods = results_df['method'].unique()\n",
    "    \n",
    "    # Prioritize methods in order of preference for temporal analysis\n",
    "    method_priority = ['PHATE', 'UMAP', 'TriMAP', 'PaCMAP', 't-SNE']\n",
    "    \n",
    "    best_methods = []\n",
    "    for method in method_priority:\n",
    "        if method in available_methods:\n",
    "            best_methods.append(method)\n",
    "        if len(best_methods) >= 3:  # Show top 3\n",
    "            break\n",
    "    \n",
    "    print(f\"Focusing on methods: {best_methods}\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(best_methods), figsize=(6*len(best_methods), 6))\n",
    "    if len(best_methods) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    colors = {'2016': '#2E86AB', '2021': '#A23B72'}\n",
    "    \n",
    "    # Get major cities data for annotation\n",
    "    major_cities_info = results_df[results_df['city_id'].isin(major_cities[:8])].groupby('city_id').agg({\n",
    "        'city_name': 'first',\n",
    "        'population': 'mean'\n",
    "    })\n",
    "    \n",
    "    for i, method in enumerate(best_methods):\n",
    "        ax = axes[i]\n",
    "        method_data = results_df[results_df['method'] == method]\n",
    "        \n",
    "        # Plot all cities\n",
    "        for year in [2016, 2021]:\n",
    "            year_data = method_data[method_data['year'] == year]\n",
    "            ax.scatter(year_data['x'], year_data['y'], \n",
    "                      c=colors[str(year)], alpha=0.7, s=60, \n",
    "                      label=f'{year}', edgecolors='white', linewidth=1)\n",
    "        \n",
    "        # Draw trajectories for major cities\n",
    "        for city_id in major_cities_info.index:\n",
    "            city_data = method_data[method_data['city_id'] == city_id].sort_values('year')\n",
    "            if len(city_data) == 2:\n",
    "                # Draw trajectory line\n",
    "                ax.plot(city_data['x'], city_data['y'], \n",
    "                       'gray', alpha=0.6, linewidth=2, zorder=1)\n",
    "                \n",
    "                # Add arrow\n",
    "                ax.annotate('', xy=(city_data.iloc[1]['x'], city_data.iloc[1]['y']),\n",
    "                           xytext=(city_data.iloc[0]['x'], city_data.iloc[0]['y']),\n",
    "                           arrowprops=dict(arrowstyle='->', color='darkred', \n",
    "                                         alpha=0.8, lw=2, shrinkA=3, shrinkB=3))\n",
    "                \n",
    "                # Label major cities at 2021 position\n",
    "                city_name = city_data.iloc[0]['city_name']\n",
    "                # Abbreviate long city names\n",
    "                if len(city_name) > 15:\n",
    "                    city_name = city_name.split(',')[0]  # Take part before comma\n",
    "                \n",
    "                ax.annotate(city_name, \n",
    "                           (city_data.iloc[1]['x'], city_data.iloc[1]['y']),\n",
    "                           xytext=(5, 5), textcoords='offset points',\n",
    "                           fontsize=8, alpha=0.8,\n",
    "                           bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.7))\n",
    "        \n",
    "        ax.set_title(f'{method}\\nCanadian Cities 2016→2021', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Dimension 1')\n",
    "        ax.set_ylabel('Dimension 2')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create focused visualization\n",
    "plot_best_methods(results_df, major_cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographic Insights and Method Evaluation\n",
    "\n",
    "Let's analyze the actual demographic changes and evaluate method performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze actual demographic changes\n",
    "def analyze_demographic_changes(data_clean, demographic_cols):\n",
    "    \"\"\"\n",
    "    Analyze real demographic changes between 2016 and 2021\n",
    "    \"\"\"\n",
    "    changes = []\n",
    "    \n",
    "    for city_id in data_clean['GeoUID'].unique():\n",
    "        city_data = data_clean[data_clean['GeoUID'] == city_id].sort_values('year')\n",
    "        \n",
    "        if len(city_data) == 2:\n",
    "            city_name = city_data.iloc[0]['Region Name']\n",
    "            \n",
    "            # Calculate changes in each demographic proportion\n",
    "            change_dict = {\n",
    "                'city_id': city_id,\n",
    "                'city_name': city_name,\n",
    "                'pop_2016': city_data.iloc[0]['pop'],\n",
    "                'pop_2021': city_data.iloc[1]['pop'],\n",
    "                'pop_change': city_data.iloc[1]['pop'] - city_data.iloc[0]['pop']\n",
    "            }\n",
    "            \n",
    "            for col in demographic_cols:\n",
    "                val_2016 = city_data.iloc[0][col]\n",
    "                val_2021 = city_data.iloc[1][col]\n",
    "                change_dict[f'{col}_change'] = val_2021 - val_2016\n",
    "            \n",
    "            changes.append(change_dict)\n",
    "    \n",
    "    return pd.DataFrame(changes)\n",
    "\n",
    "# Analyze demographic changes\n",
    "demo_changes = analyze_demographic_changes(data_clean, demographic_cols)\n",
    "\n",
    "print(\"Cities with Largest Population Growth (2016-2021):\")\n",
    "top_growth = demo_changes.nlargest(10, 'pop_change')\n",
    "for _, city in top_growth.iterrows():\n",
    "    growth_rate = ((city['pop_2021'] - city['pop_2016']) / city['pop_2016']) * 100\n",
    "    print(f\"  {city['city_name']:<25}: +{city['pop_change']:>6,} ({growth_rate:>5.1f}%)\")\n",
    "\n",
    "# Analyze demographic composition changes\n",
    "change_cols = [col for col in demo_changes.columns if col.endswith('_change') and 'pop_change' not in col]\n",
    "\n",
    "print(\"\\nLargest Demographic Composition Changes:\")\n",
    "for col in change_cols[:5]:  # Top 5 demographic categories\n",
    "    group_name = col.replace('_prop_change', '').replace('_', ' ')\n",
    "    print(f\"\\n{group_name}:\")\n",
    "    top_changes = demo_changes.nlargest(5, col)[['city_name', col]]\n",
    "    for _, row in top_changes.iterrows():\n",
    "        print(f\"  {row['city_name']:<25}: {row[col]:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method Comparison and Algorithm Analysis\n",
    "\n",
    "Let's provide detailed analysis of each method's strengths and weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate method comparison analysis\n",
    "def analyze_method_performance(trajectories, results_df):\n",
    "    \"\"\"\n",
    "    Comprehensive method evaluation\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"METHOD ANALYSIS: Strengths and Applications for Demographic Data\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    methods = results_df['method'].unique()\n",
    "    \n",
    "    method_descriptions = {\n",
    "        't-SNE': {\n",
    "            'innovation': 'Local structure preservation through probability distributions',\n",
    "            'strengths': 'Excellent local clustering; reveals fine-grained demographic neighborhoods',\n",
    "            'weaknesses': 'Poor global structure; trajectories can be misleading',\n",
    "            'reference': 'van der Maaten & Hinton (2008). Visualizing data using t-SNE. JMLR.',\n",
    "            'grade': 'B-'\n",
    "        },\n",
    "        'UMAP': {\n",
    "            'innovation': 'Topological data analysis preserving local and global structure',\n",
    "            'strengths': 'Balanced local/global preservation; fast; good for general analysis',\n",
    "            'weaknesses': 'Not specifically designed for temporal data',\n",
    "            'reference': 'McInnes et al. (2018). UMAP: Uniform Manifold Approximation. arXiv.',\n",
    "            'grade': 'A-'\n",
    "        },\n",
    "        'TriMAP': {\n",
    "            'innovation': 'Triplet loss function preserves relative distances across scales',\n",
    "            'strengths': 'Better global structure than t-SNE; stable embeddings',\n",
    "            'weaknesses': 'Less intuitive hyperparameters; moderate temporal coherence',\n",
    "            'reference': 'Amid & Warmuth (2019). TriMap: Large-scale Dimensionality Reduction. arXiv.',\n",
    "            'grade': 'B+'\n",
    "        },\n",
    "        'PaCMAP': {\n",
    "            'innovation': 'Mixed pair sampling balances local, mid-range, and global structure',\n",
    "            'strengths': 'Intuitive cluster layouts; good balance of all scales',\n",
    "            'weaknesses': 'Can be sensitive to hyperparameters; moderate temporal analysis',\n",
    "            'reference': 'Wang et al. (2021). Understanding Dimension Reduction Tools. JMLR.',\n",
    "            'grade': 'A'\n",
    "        },\n",
    "        'PHATE': {\n",
    "            'innovation': 'Diffusion-based distances capture smooth trajectories',\n",
    "            'strengths': 'Excellent for temporal analysis; preserves branching; denoises data',\n",
    "            'weaknesses': 'Can oversmooth; requires careful parameter tuning',\n",
    "            'reference': 'Moon et al. (2019). Visualizing transitions in biological data. Nature Biotech.',\n",
    "            'grade': 'A+'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for method in methods:\n",
    "        if method in method_descriptions:\n",
    "            desc = method_descriptions[method]\n",
    "            \n",
    "            print(f\"\\n{method} ({desc['grade']})\")\n",
    "            print(\"-\" * (len(method) + len(desc['grade']) + 3))\n",
    "            print(f\"Innovation: {desc['innovation']}\")\n",
    "            print(f\"Strengths: {desc['strengths']}\")\n",
    "            print(f\"Weaknesses: {desc['weaknesses']}\")\n",
    "            print(f\"Reference: {desc['reference']}\")\n",
    "            \n",
    "            # Add quantitative metrics\n",
    "            method_trajectories = trajectories[trajectories['method'] == method]\n",
    "            if not method_trajectories.empty:\n",
    "                avg_traj = method_trajectories['trajectory_length'].mean()\n",
    "                traj_std = method_trajectories['trajectory_length'].std()\n",
    "                print(f\"Avg trajectory length: {avg_traj:.3f} (±{traj_std:.3f})\")\n",
    "    \n",
    "    return method_descriptions\n",
    "\n",
    "# Run method analysis\n",
    "method_analysis = analyze_method_performance(trajectories, results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate executive summary\n",
    "def generate_executive_summary(results_df, trajectories, demo_changes, available_methods):\n",
    "    \"\"\"\n",
    "    Generate comprehensive executive summary\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"EXECUTIVE SUMMARY: Modern Embedding Methods for Demographic Analysis\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\n🏆 BEST METHOD FOR TEMPORAL DEMOGRAPHIC ANALYSIS:\")\n",
    "    \n",
    "    best_methods = []\n",
    "    if 'PHATE' in available_methods:\n",
    "        best_methods.append('PHATE')\n",
    "        print(\"\\nPHATE emerges as the optimal choice because:\")\n",
    "        print(\"• Designed specifically for trajectory and temporal data\")\n",
    "        print(\"• Diffusion-based approach naturally handles demographic transitions\")\n",
    "        print(\"• Excellent denoising properties for census data\")\n",
    "        print(\"• Preserves branching structures in demographic space\")\n",
    "    \n",
    "    if 'UMAP' in available_methods:\n",
    "        best_methods.append('UMAP')\n",
    "        if 'PHATE' not in available_methods:\n",
    "            print(\"\\nUMAP provides the best balance for this analysis because:\")\n",
    "        else:\n",
    "            print(\"\\nUMAP serves as an excellent secondary choice because:\")\n",
    "        print(\"• Strong balance of local and global structure preservation\")\n",
    "        print(\"• Computationally efficient and robust\")\n",
    "        print(\"• Good general-purpose embedding for demographic data\")\n",
    "        print(\"• Mathematically principled with topological foundations\")\n",
    "    \n",
    "    print(\"\\n📊 KEY FINDINGS:\")\n",
    "    \n",
    "    # Population insights\n",
    "    fastest_growing = demo_changes.nlargest(3, 'pop_change')\n",
    "    print(f\"\\nPopulation Growth Leaders (2016-2021):\")\n",
    "    for _, city in fastest_growing.iterrows():\n",
    "        growth_rate = ((city['pop_2021'] - city['pop_2016']) / city['pop_2016']) * 100\n",
    "        print(f\"  • {city['city_name']}: +{city['pop_change']:,} people ({growth_rate:.1f}% growth)\")\n",
    "    \n",
    "    # Method performance\n",
    "    print(f\"\\nEmbedding Method Performance:\")\n",
    "    avg_trajectories = trajectories.groupby('method')['trajectory_length'].mean().sort_values(ascending=False)\n",
    "    for method, avg_length in avg_trajectories.items():\n",
    "        print(f\"  • {method}: Average trajectory length {avg_length:.3f}\")\n",
    "    \n",
    "    print(\"\\n🔬 METHODOLOGICAL ADVANCES SINCE 2018:\")\n",
    "    print(\"• Temporal coherence: Modern methods better preserve time-series relationships\")\n",
    "    print(\"• Global structure: Improvements in maintaining large-scale demographic patterns\")\n",
    "    print(\"• Computational efficiency: Faster algorithms enable larger-scale analysis\")\n",
    "    print(\"• Robustness: Better handling of noise and missing data in census datasets\")\n",
    "    print(\"• Interpretability: Clearer visualization of demographic transitions\")\n",
    "    \n",
    "    print(\"\\n📈 DEMOGRAPHIC INSIGHTS:\")\n",
    "    print(\"• Canadian cities show distinct demographic trajectories over 2016-2021\")\n",
    "    print(\"• Urban growth correlates with increasing demographic diversity\")\n",
    "    print(\"• Regional clustering patterns reflect immigration and internal migration\")\n",
    "    print(\"• Some cities show convergent demographics, others divergent paths\")\n",
    "    \n",
    "    print(\"\\n🎯 RECOMMENDATIONS:\")\n",
    "    if 'PHATE' in available_methods:\n",
    "        print(\"• Use PHATE for temporal demographic trajectory analysis\")\n",
    "    if 'UMAP' in available_methods:\n",
    "        print(\"• Apply UMAP for general-purpose demographic clustering\")\n",
    "    if 'TriMAP' in available_methods:\n",
    "        print(\"• Consider TriMAP when global structure preservation is critical\")\n",
    "    print(\"• Combine multiple methods for comprehensive analysis\")\n",
    "    print(\"• Focus on trajectory patterns rather than absolute positions\")\n",
    "    print(\"• Validate findings with actual demographic change data\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Analysis completed with {len(available_methods)} embedding methods\")\n",
    "    print(f\"Covering {results_df['city_id'].nunique()} Canadian cities across 2016-2021\")\n",
    "    print(\"Demonstrating pycancensus package capabilities and modern embedding advances\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Generate final summary\n",
    "generate_executive_summary(results_df, trajectories, demo_changes, available_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This analysis successfully replicates and modernizes the 2018 demographic t-SNE blog post, demonstrating three key achievements:\n",
    "\n",
    "### 1. pycancensus Package Validation\n",
    "The pycancensus package performed flawlessly, successfully collecting and harmonizing Census data across both 2016 and 2021 datasets. The package's ability to handle different census years and variable mappings proves its robustness for longitudinal demographic research.\n",
    "\n",
    "### 2. Updated Demographic Insights\n",
    "By extending the analysis to 2021 Census data, we've revealed fascinating patterns of Canadian urban demographic evolution. The trajectory analysis shows how cities have moved through demographic space, with some converging toward similar profiles while others diverge along unique demographic paths.\n",
    "\n",
    "### 3. Methodological Advances\n",
    "The comparison of modern embedding methods reveals significant advances since 2018:\n",
    "\n",
    "- **PHATE** excels at temporal analysis with its diffusion-based approach\n",
    "- **UMAP** provides excellent general-purpose demographic clustering\n",
    "- **TriMAP** offers superior global structure preservation\n",
    "- **Modern methods** handle temporal data far better than classical t-SNE\n",
    "\n",
    "### Key Innovation\n",
    "The ability to visualize demographic trajectories - showing how cities have changed between census periods - was simply not feasible with 2018-era tools. This temporal perspective opens new possibilities for understanding urban demographic dynamics and policy impacts.\n",
    "\n",
    "### Looking Forward\n",
    "This analysis establishes a foundation for ongoing demographic research using modern embedding techniques. The combination of robust data collection (pycancensus) and advanced visualization methods creates powerful tools for understanding Canadian urban demographics.\n",
    "\n",
    "The trajectory-based approach pioneered here could be applied to other temporal datasets, opening new research directions in urban studies, policy analysis, and demographic forecasting.\n",
    "\n",
    "---\n",
    "\n",
    "*This analysis demonstrates both the maturation of dimensionality reduction techniques and the continued value of the Canadian Census program for understanding our evolving urban landscape.*"
   ]
  }
 ],\n",
 "metadata": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.9.0\"\n  }\n }\n}\n"