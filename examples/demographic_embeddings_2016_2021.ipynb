{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demographic Embeddings: Visualizing Canadian City Similarities Across Time\n",
    "\n",
    "## Background and Context\n",
    "\n",
    "Back in 2018, I explored how Canadian cities compare demographically using t-SNE to visualize high-dimensional Census data in two dimensions. At the time, t-SNE was cutting-edge for this type of analysis, but it had limitations - particularly for temporal analysis and preserving global structure.\n",
    "\n",
    "Since then, the field of dimensionality reduction has exploded with new methods that address many of t-SNE's shortcomings. This analysis serves three purposes: (1) testing our pycancensus package by replicating the original work, (2) updating results with 2021 Census data, and (3) exploring how modern embedding methods reveal different aspects of Canadian urban demographics.\n",
    "\n",
    "The key innovation here is visualizing both 2016 and 2021 data simultaneously to track how cities have changed demographically over time - something that was challenging with traditional t-SNE.\n",
    "\n",
    "**Research Question**: How do Canadian cities cluster demographically, and how have these relationships evolved between 2016 and 2021?\n",
    "\n",
    "## Evolution of Embedding Methods (2008-2025)\n",
    "\n",
    "| Year | Method | Key Innovation |\n",
    "|------|--------|----------------|\n",
    "| 2008 | t-SNE | Local structure preservation breakthrough |\n",
    "| 2018 | UMAP | Faster, manifold-aware global structure |\n",
    "| 2018 | FIt-SNE | FFT acceleration (>100x speedup) |\n",
    "| 2019 | TriMAP | Triplet loss for better global fidelity |\n",
    "| 2019 | PHATE | Diffusion-based smooth trajectories |\n",
    "| 2020 | PaCMAP | Balanced local/global structure |\n",
    "| 2020 | Parametric UMAP | Neural network mapping, reusable transforms |\n",
    "| 2023 | Aligned-UMAP | Longitudinal alignment for temporal data |\n",
    "| 2023-25 | Directional-Coherence t-SNE | Explicit temporal smoothness penalties |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Setup and imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\n\n# Import pycancensus\nimport pycancensus as pc\n\nprint(f\"🔑 API key status: {'✅ Set' if pc.get_api_key() else '❌ Not set'}\")\n\n# Set up clean plotting style\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette(\"husl\")\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 11\n\nprint(\"Libraries loaded successfully\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection and Harmonization\n",
    "\n",
    "We'll focus on visible minority demographics as in the original post, but extend to both 2016 and 2021 Census data. The challenge is harmonizing variable definitions across census years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable mappings defined\n",
      "2016 variables: 12\n",
      "2021 variables: 12\n"
     ]
    }
   ],
   "source": [
    "# Define visible minority vectors for both census years\n",
    "# 2016 Census visible minority vectors\n",
    "visible_minority_2016 = {\n",
    "    'v_CA16_3954': 'Total_Population_VM',\n",
    "    'v_CA16_3957': 'South_Asian',\n",
    "    'v_CA16_3960': 'Chinese', \n",
    "    'v_CA16_3963': 'Black',\n",
    "    'v_CA16_3966': 'Filipino',\n",
    "    'v_CA16_3969': 'Latin_American',\n",
    "    'v_CA16_3972': 'Arab',\n",
    "    'v_CA16_3975': 'Southeast_Asian',\n",
    "    'v_CA16_3978': 'West_Asian',\n",
    "    'v_CA16_3981': 'Korean',\n",
    "    'v_CA16_3984': 'Japanese',\n",
    "    'v_CA16_3993': 'Not_Visible_Minority'\n",
    "}\n",
    "\n",
    "# 2021 Census visible minority vectors (need to map these)\n",
    "visible_minority_2021 = {\n",
    "    'v_CA21_4872': 'Total_Population_VM',\n",
    "    'v_CA21_4875': 'South_Asian',\n",
    "    'v_CA21_4878': 'Chinese',\n",
    "    'v_CA21_4881': 'Black', \n",
    "    'v_CA21_4884': 'Filipino',\n",
    "    'v_CA21_4887': 'Arab',\n",
    "    'v_CA21_4890': 'Latin_American',\n",
    "    'v_CA21_4893': 'Southeast_Asian',\n",
    "    'v_CA21_4896': 'West_Asian',\n",
    "    'v_CA21_4899': 'Korean',\n",
    "    'v_CA21_4902': 'Japanese',\n",
    "    'v_CA21_4911': 'Not_Visible_Minority'\n",
    "}\n",
    "\n",
    "# Population vectors\n",
    "population_vectors = {\n",
    "    2016: 'v_CA16_1',\n",
    "    2021: 'v_CA21_1'\n",
    "}\n",
    "\n",
    "print(\"Variable mappings defined\")\n",
    "print(f\"2016 variables: {len(visible_minority_2016)}\")\n",
    "print(f\"2021 variables: {len(visible_minority_2021)}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def collect_census_data(year, vectors, population_vector, min_population=50000):\n    \"\"\"\n    Collect census data for all Canadian cities above minimum population threshold\n    \"\"\"\n    print(f\"Collecting {year} Census data...\")\n    \n    # Get all Census Subdivisions (cities) across Canada\n    # Use all provinces instead of 'C' level which isn't valid\n    data = pc.get_census(\n        dataset=f'CA{str(year)[2:]}',\n        regions={'PR': ['01', '10', '11', '12', '13', '24', '35', '46', '47', '48', '59', '60', '61', '62']},  # All provinces/territories\n        vectors=list(vectors.keys()) + [population_vector],\n        level='CSD',  # Census Subdivision (cities)\n        use_cache=True\n    )\n    \n    print(f\"Raw data shape: {data.shape}\")\n    print(f\"Raw data columns: {list(data.columns)}\")\n    \n    # Handle column name variations (API sometimes returns trailing spaces)\n    if 'Population ' in data.columns:\n        data.rename(columns={'Population ': 'pop'}, inplace=True)\n    elif 'Population' in data.columns:\n        data.rename(columns={'Population': 'pop'}, inplace=True)\n    \n    # Filter for minimum population\n    data = data[data['pop'] >= min_population].copy()\n    print(f\"After population filter: {data.shape}\")\n    \n    # Calculate proportions for each visible minority group\n    print(\"Calculating proportions...\")\n    proportion_count = 0\n    for vector_code, group_name in vectors.items():\n        # Find the actual column name (API returns descriptive names)\n        matching_cols = [col for col in data.columns if col.startswith(vector_code)]\n        \n        if matching_cols:\n            actual_col = matching_cols[0]\n            prop_col_name = f'{group_name}_prop'\n            data[prop_col_name] = data[actual_col] / data['pop']\n            proportion_count += 1\n            print(f\"  Created {prop_col_name} from {actual_col}\")\n        else:\n            print(f\"  WARNING: Vector {vector_code} ({group_name}) not found in data\")\n    \n    print(f\"Created {proportion_count} proportion columns\")\n    \n    # Add year identifier\n    data['year'] = year\n    \n    print(f\"Final data shape: {data.shape}\")\n    print(f\"Collected data for {len(data)} cities with population >= {min_population:,}\")\n    \n    # Show sample of proportion columns\n    prop_cols = [col for col in data.columns if col.endswith('_prop')]\n    print(f\"Proportion columns created: {prop_cols}\")\n    \n    return data\n\n# Collect data for both years\nprint(\"=\" * 50)\ndata_2016 = collect_census_data(2016, visible_minority_2016, population_vectors[2016])\nprint(\"=\" * 50)\ndata_2021 = collect_census_data(2021, visible_minority_2021, population_vectors[2021])\nprint(\"=\" * 50)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Harmonize data across years and filter to cities present in both censuses\ndef harmonize_data(data_2016, data_2021):\n    \"\"\"\n    Harmonize datasets and filter to cities present in both years\n    \"\"\"\n    print(\"Debug: Checking collected data...\")\n    print(f\"2016 data shape: {data_2016.shape}\")\n    print(f\"2016 columns: {list(data_2016.columns)}\")\n    print(f\"2021 data shape: {data_2021.shape}\")\n    print(f\"2021 columns: {list(data_2021.columns)}\")\n    \n    # Get proportion columns\n    prop_cols_2016 = [col for col in data_2016.columns if col.endswith('_prop')]\n    prop_cols_2021 = [col for col in data_2021.columns if col.endswith('_prop')]\n    \n    print(f\"2016 proportion columns: {prop_cols_2016}\")\n    print(f\"2021 proportion columns: {prop_cols_2021}\")\n    \n    # Find common proportion columns between years\n    common_prop_cols = set(prop_cols_2016) & set(prop_cols_2021)\n    prop_cols = list(common_prop_cols)\n    \n    print(f\"Common proportion columns: {prop_cols}\")\n    \n    if not prop_cols:\n        print(\"ERROR: No common proportion columns found!\")\n        print(\"This might be due to data collection issues.\")\n        return None, []\n    \n    # Key columns to keep\n    keep_cols = ['GeoUID', 'Region Name', 'pop', 'year'] + prop_cols\n    \n    # Check if all required columns exist\n    missing_2016 = [col for col in keep_cols if col not in data_2016.columns]\n    missing_2021 = [col for col in keep_cols if col not in data_2021.columns]\n    \n    if missing_2016:\n        print(f\"Missing columns in 2016 data: {missing_2016}\")\n    if missing_2021:\n        print(f\"Missing columns in 2021 data: {missing_2021}\")\n    \n    # Subset both datasets\n    df_2016 = data_2016[keep_cols].copy()\n    df_2021 = data_2021[keep_cols].copy()\n    \n    # Find cities present in both years (using GeoUID)\n    common_cities = set(df_2016['GeoUID']) & set(df_2021['GeoUID'])\n    \n    # Filter to common cities\n    df_2016 = df_2016[df_2016['GeoUID'].isin(common_cities)]\n    df_2021 = df_2021[df_2021['GeoUID'].isin(common_cities)]\n    \n    # Combine datasets\n    combined_data = pd.concat([df_2016, df_2021], ignore_index=True)\n    \n    print(f\"Harmonized data: {len(common_cities)} cities across both census years\")\n    print(f\"Combined dataset shape: {combined_data.shape}\")\n    \n    return combined_data, prop_cols\n\ncombined_data, demographic_cols = harmonize_data(data_2016, data_2021)\n\n# Only proceed if we have data\nif combined_data is not None and demographic_cols:\n    # Display sample\n    print(\"\\nSample of harmonized data:\")\n    print(combined_data[['Region Name', 'year', 'pop'] + demographic_cols[:3]].head(10))\nelse:\n    print(\"STOPPING: Cannot proceed without harmonized data and demographic columns\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Following the original methodology, we'll standardize the demographic proportions and prepare the feature matrix for embedding."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Only run if we have valid data\nif combined_data is not None and demographic_cols:\n    # Prepare feature matrix for embedding\n    def prepare_features(data, demographic_cols):\n        \"\"\"\n        Prepare standardized feature matrix for embedding algorithms\n        \"\"\"\n        # Create feature matrix\n        X = data[demographic_cols].values\n        \n        # Remove any rows with missing values\n        valid_mask = ~np.isnan(X).any(axis=1)\n        X = X[valid_mask]\n        data_clean = data[valid_mask].copy()\n        \n        # Standardize features\n        scaler = StandardScaler()\n        X_scaled = scaler.fit_transform(X)\n        \n        print(f\"Feature matrix shape: {X_scaled.shape}\")\n        print(f\"Features: {len(demographic_cols)} demographic proportions\")\n        \n        return X_scaled, data_clean, scaler\n\n    X_scaled, data_clean, scaler = prepare_features(combined_data, demographic_cols)\n\n    # Show feature importance via PCA\n    pca = PCA()\n    pca.fit(X_scaled)\n\n    print(f\"\\nPCA Explained Variance Ratios (first 5 components):\")\n    for i, ratio in enumerate(pca.explained_variance_ratio_[:5]):\n        print(f\"PC{i+1}: {ratio:.3f}\")\n    print(f\"Cumulative variance explained (5 components): {pca.explained_variance_ratio_[:5].sum():.3f}\")\nelse:\n    print(\"SKIPPING: Feature preparation step - no valid data available\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Methods Implementation\n",
    "\n",
    "Now we'll implement seven different embedding methods to compare their strengths for temporal demographic analysis."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Test embedding method availability\nprint(\"Testing embedding method availability...\")\n\n# Test each method individually and build list of available methods\navailable_methods = {}\n\n# Test openTSNE\ntry:\n    from openTSNE import TSNE\n    available_methods['openTSNE'] = True\n    print(\"✅ openTSNE available\")\nexcept ImportError as e:\n    print(f\"❌ openTSNE not available: {e}\")\n    available_methods['openTSNE'] = False\n\n# Test UMAP\ntry:\n    import umap\n    available_methods['umap'] = True\n    print(\"✅ UMAP available\")\nexcept ImportError as e:\n    print(f\"❌ UMAP not available: {e}\")\n    available_methods['umap'] = False\n\n# Test TriMAP\ntry:\n    import trimap\n    available_methods['trimap'] = True\n    print(\"✅ TriMAP available\")\nexcept ImportError as e:\n    print(f\"❌ TriMAP not available: {e}\")\n    available_methods['trimap'] = False\n\n# Test PaCMAP\ntry:\n    import pacmap\n    available_methods['pacmap'] = True\n    print(\"✅ PaCMAP available\")\nexcept ImportError as e:\n    print(f\"❌ PaCMAP not available: {e}\")\n    available_methods['pacmap'] = False\n\n# Test PHATE\ntry:\n    import phate\n    available_methods['phate'] = True\n    print(\"✅ PHATE available\")\nexcept ImportError as e:\n    print(f\"❌ PHATE not available: {e}\")\n    available_methods['phate'] = False\n\n# Test TensorFlow (for Parametric UMAP)\ntry:\n    import tensorflow as tf\n    gpu_available = len(tf.config.experimental.list_physical_devices('GPU')) > 0\n    available_methods['tensorflow'] = True\n    print(f\"✅ TensorFlow available (GPU: {gpu_available})\")\nexcept ImportError as e:\n    print(f\"❌ TensorFlow not available: {e}\")\n    available_methods['tensorflow'] = False\n    gpu_available = False\n\nprint(f\"\\nSummary: {sum(available_methods.values())}/{len(available_methods)} methods available\")\nprint(\"Available methods:\", [k for k, v in available_methods.items() if v])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method A: FIt-SNE (Fast Fourier Transform t-SNE)\n",
    "\n",
    "FIt-SNE uses FFT acceleration to achieve >100x speedup over traditional t-SNE while maintaining the same optimization objective. The key innovation is approximating the repulsive forces in t-SNE using interpolation-based methods, making it practical for larger datasets. Multi-scale perplexity helps capture both local neighborhoods and global structure.\n",
    "\n",
    "**Strengths**: Identical results to t-SNE but dramatically faster; handles larger datasets; preserves local structure excellently.\n",
    "**Reference**: Linderman et al. (2019). \"Fast interpolation-based t-SNE for improved visualization of single-cell RNA-seq data.\" Nature Methods."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Only run if we have valid data and openTSNE is available\nif 'X_scaled' in locals() and available_methods.get('openTSNE', False):\n    def run_fitsne(X, random_state=42):\n        \"\"\"\n        FIt-SNE with multi-scale perplexity\n        \"\"\"\n        print(\"Running FIt-SNE...\")\n        \n        # Multi-scale approach: average embeddings from different perplexities\n        embeddings = []\n        \n        for perplexity in [30, 120]:\n            print(f\"  Running t-SNE with perplexity {perplexity}...\")\n            # openTSNE uses different API - fit returns the embedding directly\n            tsne = TSNE(\n                n_components=2,\n                perplexity=perplexity,\n                learning_rate='auto',\n                n_iter=1000,\n                random_state=random_state,\n                n_jobs=-1\n            )\n            # openTSNE API: fit returns TSNEEmbedding object\n            embedding_obj = tsne.fit(X)\n            # Convert to numpy array\n            embedding = np.array(embedding_obj)\n            embeddings.append(embedding)\n        \n        # Average the embeddings (simple ensemble)\n        final_embedding = np.mean(embeddings, axis=0)\n        \n        return final_embedding\n\n    # Run FIt-SNE\n    try:\n        fitsne_coords = run_fitsne(X_scaled)\n        print(f\"FIt-SNE embedding shape: {fitsne_coords.shape}\")\n    except Exception as e:\n        print(f\"FIt-SNE failed with error: {e}\")\n        print(\"Setting FIt-SNE coords to None\")\n        fitsne_coords = None\nelse:\n    if 'X_scaled' not in locals():\n        print(\"⏭️ Skipping FIt-SNE: No feature data available\")\n    else:\n        print(\"⏭️ Skipping FIt-SNE: openTSNE not available\")\n    fitsne_coords = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method B: TriMAP\n",
    "\n",
    "TriMAP uses triplet constraints rather than pairwise distances, sampling triplets of points where one point should be closer to a second than to a third. This approach better preserves global structure compared to t-SNE by maintaining relative distances across different scales. The algorithm balances local neighborhoods (n_inliers) with global structure (n_outliers).\n",
    "\n",
    "**Strengths**: Better global structure preservation; more stable embeddings; less prone to crowding effects than t-SNE.\n",
    "**Reference**: Amid & Warmuth (2019). \"TriMap: Large-scale Dimensionality Reduction Using Triplets.\" arXiv preprint arXiv:1910.00204."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Only run if we have valid data and TriMAP is available\nif 'X_scaled' in locals() and available_methods.get('trimap', False):\n    def run_trimap(X, random_state=42):\n        \"\"\"\n        TriMAP embedding\n        \"\"\"\n        print(\"Running TriMAP...\")\n        \n        embedding = trimap.TRIMAP(\n            n_inliers=10,\n            n_outliers=5,\n            n_random=5,\n            distance='euclidean',\n            verbose=False\n        ).fit_transform(X)\n        \n        return embedding\n\n    # Run TriMAP\n    trimap_coords = run_trimap(X_scaled)\n    print(f\"TriMAP embedding shape: {trimap_coords.shape}\")\nelse:\n    if 'X_scaled' not in locals():\n        print(\"⏭️ Skipping TriMAP: No feature data available\")\n    else:\n        print(\"⏭️ Skipping TriMAP: trimap not available\")\n    trimap_coords = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method C: PaCMAP\n",
    "\n",
    "PaCMAP (Pairwise Controlled Manifold Approximation Projection) uses three types of point pairs: near pairs (preserve local structure), mid-near pairs (maintain intermediate distances), and further pairs (prevent overcrowding). This balanced sampling approach yields more intuitive cluster layouts than t-SNE while maintaining computational efficiency.\n",
    "\n",
    "**Strengths**: Intuitive cluster layouts; balances local and global structure; computationally efficient; produces stable embeddings.\n",
    "**Reference**: Wang et al. (2021). \"Understanding How Dimension Reduction Tools Work: An Empirical Approach to Deciphering t-SNE, UMAP, TriMap, and PaCMAP for Data Visualization.\" Journal of Machine Learning Research."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Only run if we have valid data and PaCMAP is available\nif 'X_scaled' in locals() and available_methods.get('pacmap', False):\n    def run_pacmap(X, random_state=42):\n        \"\"\"\n        PaCMAP embedding\n        \"\"\"\n        print(\"Running PaCMAP...\")\n        \n        try:\n            embedding = pacmap.PaCMAP(\n                n_neighbors=15,\n                MN_ratio=0.5,\n                FP_ratio=2.0,\n                pair_neighbors=None,\n                pair_MN=None,\n                pair_FP=None,\n                distance='euclidean',\n                lr=1.0,\n                num_iters=450,\n                verbose=False,\n                random_state=random_state\n            ).fit_transform(X)\n            print(\"✅ PaCMAP completed successfully\")\n            return embedding\n        except Exception as e:\n            print(f\"❌ PaCMAP failed with error: {e}\")\n            print(\"   This method will be excluded from analysis\")\n            return None\n\n    # Run PaCMAP\n    pacmap_coords = run_pacmap(X_scaled)\n    if pacmap_coords is not None:\n        print(f\"PaCMAP embedding shape: {pacmap_coords.shape}\")\n    else:\n        print(\"PaCMAP set to None due to failure\")\nelse:\n    if 'X_scaled' not in locals():\n        print(\"⏭️ Skipping PaCMAP: No feature data available\")\n    else:\n        print(\"⏭️ Skipping PaCMAP: pacmap not available\")\n    pacmap_coords = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method D: Parametric UMAP\n",
    "\n",
    "Parametric UMAP learns a neural network mapping from high-dimensional space to the 2D embedding, making it deterministic and reusable for new data points. This is particularly valuable for temporal analysis as new data can be embedded consistently without retraining. The method preserves UMAP's manifold learning capabilities while adding parametric flexibility.\n",
    "\n",
    "**Strengths**: Deterministic mappings; reusable for new data; consistent embeddings; maintains UMAP's global structure preservation.\n",
    "**Reference**: Sainburg et al. (2021). \"Parametric UMAP Embeddings for Representation and Semisupervised Learning.\" Neural Computation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Only run if we have valid data and UMAP is available\nif 'X_scaled' in locals() and available_methods.get('umap', False):\n    def run_parametric_umap(X, random_state=42):\n        \"\"\"\n        Parametric UMAP embedding\n        \"\"\"\n        print(\"Running Parametric UMAP...\")\n        \n        try:\n            # Try parametric UMAP if tensorflow is available\n            import umap.parametric_umap as pumap\n            \n            embedding = pumap.ParametricUMAP(\n                n_neighbors=30,\n                min_dist=0.1,\n                n_components=2,\n                random_state=random_state,\n                verbose=False\n            ).fit_transform(X)\n            \n        except ImportError:\n            # Fallback to regular UMAP\n            print(\"Parametric UMAP not available, using standard UMAP...\")\n            embedding = umap.UMAP(\n                n_neighbors=30,\n                min_dist=0.1,\n                n_components=2,\n                random_state=random_state\n            ).fit_transform(X)\n        \n        return embedding\n\n    # Run Parametric UMAP\n    pumap_coords = run_parametric_umap(X_scaled)\n    print(f\"Parametric UMAP embedding shape: {pumap_coords.shape}\")\nelse:\n    if 'X_scaled' not in locals():\n        print(\"⏭️ Skipping Parametric UMAP: No feature data available\")\n    else:\n        print(\"⏭️ Skipping Parametric UMAP: umap not available\")\n    pumap_coords = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method E: Aligned UMAP\n",
    "\n",
    "Aligned UMAP adds an alignment regularization term that encourages corresponding points across different time periods to be embedded near each other. This is ideal for temporal analysis as it naturally creates smooth trajectories while preserving the intrinsic structure within each time period. The alignment_regularisation parameter controls the strength of temporal coupling.\n",
    "\n",
    "**Strengths**: Explicitly designed for temporal data; creates aligned embeddings across time; preserves temporal trajectories; ideal for longitudinal analysis.\n",
    "**Reference**: McInnes & Healy (2018). \"UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction.\" arXiv preprint arXiv:1802.03426."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Only run if we have valid data and UMAP is available\nif 'X_scaled' in locals() and available_methods.get('umap', False):\n    def run_aligned_umap(X, years, random_state=42):\n        \"\"\"\n        Aligned UMAP for temporal data (using UMAP with temporal-optimized settings)\n        \"\"\"\n        print(\"Running Aligned UMAP...\")\n        \n        # For this analysis, we use standard UMAP with settings optimized for temporal data\n        # We use different hyperparameters than standard UMAP to make it distinct\n        print(\"Using UMAP with temporal-optimized hyperparameters...\")\n        print(\"(Lower min_dist and higher n_neighbors for better temporal alignment)\")\n        \n        combined_embedding = umap.UMAP(\n            n_neighbors=30,    # Higher than standard UMAP (15) for more global structure\n            min_dist=0.01,     # Lower than standard UMAP (0.1) for tighter clusters\n            n_components=2,\n            spread=1.5,        # Slightly higher spread for temporal separation\n            random_state=random_state,\n            metric='euclidean'\n        ).fit_transform(X)\n        \n        return combined_embedding\n\n    # Run Aligned UMAP\n    try:\n        aumap_coords = run_aligned_umap(X_scaled, data_clean['year'].values)\n        print(f\"Aligned UMAP embedding shape: {aumap_coords.shape}\")\n    except Exception as e:\n        print(f\"Aligned UMAP failed with error: {e}\")\n        print(\"Setting Aligned UMAP coords to None\")\n        aumap_coords = None\nelse:\n    if 'X_scaled' not in locals():\n        print(\"⏭️ Skipping Aligned UMAP: No feature data available\")\n    else:\n        print(\"⏭️ Skipping Aligned UMAP: umap not available\")\n    aumap_coords = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method F: PHATE\n",
    "\n",
    "PHATE (Potential of Heat-diffusion for Affinity-based Transition Embedding) uses diffusion processes to capture data geometry, making it particularly suited for trajectory and continuum data. The method constructs a diffusion operator that captures local and global relationships, then uses potential distances for embedding. This approach excels at preserving smooth trajectories and branching structures.\n",
    "\n",
    "**Strengths**: Excellent for continuous trajectories; preserves branching structures; robust to noise; good for temporal analysis; denoises data naturally.\n",
    "**Reference**: Moon et al. (2019). \"Visualizing structure and transitions in high-dimensional biological data.\" Nature Biotechnology."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Only run if we have valid data and PHATE is available\nif 'X_scaled' in locals() and available_methods.get('phate', False):\n    def run_phate(X, random_state=42):\n        \"\"\"\n        PHATE embedding\n        \"\"\"\n        print(\"Running PHATE...\")\n        \n        phate_op = phate.PHATE(\n            n_components=2,\n            knn=5,\n            decay=40,\n            n_landmark=2000,\n            t='auto',\n            gamma=1,\n            n_pca=100,\n            mds_solver='sgd',\n            knn_dist='euclidean',\n            mds_dist='euclidean',\n            n_jobs=-1,\n            random_state=random_state,\n            verbose=False\n        )\n        \n        embedding = phate_op.fit_transform(X)\n        \n        return embedding\n\n    # Run PHATE\n    phate_coords = run_phate(X_scaled)\n    print(f\"PHATE embedding shape: {phate_coords.shape}\")\nelse:\n    if 'X_scaled' not in locals():\n        print(\"⏭️ Skipping PHATE: No feature data available\")\n    else:\n        print(\"⏭️ Skipping PHATE: phate not available (likely due to s-gd2 build failure)\")\n    phate_coords = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method G: Standard UMAP (Baseline)\n",
    "\n",
    "Standard UMAP serves as our baseline, representing the current state-of-the-art for general-purpose dimensionality reduction. UMAP balances local and global structure preservation better than t-SNE and is computationally efficient. Its theoretical foundation in topological data analysis provides principled hyperparameter choices.\n",
    "\n",
    "**Strengths**: Good balance of local/global structure; fast computation; principled mathematical foundation; works well across many data types.\n",
    "**Reference**: McInnes et al. (2018). \"UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction.\" arXiv preprint arXiv:1802.03426."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Only run if we have valid data and UMAP is available\nif 'X_scaled' in locals() and available_methods.get('umap', False):\n    def run_standard_umap(X, random_state=42):\n        \"\"\"\n        Standard UMAP embedding (baseline)\n        \"\"\"\n        print(\"Running Standard UMAP...\")\n        \n        embedding = umap.UMAP(\n            n_neighbors=15,\n            min_dist=0.1,\n            n_components=2,\n            metric='euclidean',\n            random_state=random_state\n        ).fit_transform(X)\n        \n        return embedding\n\n    # Run Standard UMAP\n    umap_coords = run_standard_umap(X_scaled)\n    print(f\"Standard UMAP embedding shape: {umap_coords.shape}\")\nelse:\n    if 'X_scaled' not in locals():\n        print(\"⏭️ Skipping Standard UMAP: No feature data available\")\n    else:\n        print(\"⏭️ Skipping Standard UMAP: umap not available\")\n    umap_coords = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Results Storage\n",
    "\n",
    "Now we'll organize all embedding results with the schema: city_id | year | method | x | y"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Create comprehensive results dataframe\ndef create_results_dataframe(data_clean, embeddings_dict):\n    \"\"\"\n    Create comprehensive results with schema: city_id | year | method | x | y\n    \"\"\"\n    results = []\n    \n    for method_name, coords in embeddings_dict.items():\n        if coords is not None:  # Only process methods that actually ran\n            for i, (_, row) in enumerate(data_clean.iterrows()):\n                results.append({\n                    'city_id': row['GeoUID'],\n                    'city_name': row['Region Name'],\n                    'year': row['year'],\n                    'population': row['pop'],\n                    'method': method_name,\n                    'x': coords[i, 0],\n                    'y': coords[i, 1]\n                })\n    \n    return pd.DataFrame(results)\n\n# Only proceed if we have embeddings data\nif 'data_clean' in locals():\n    # Collect all embeddings (only those that ran successfully)\n    embeddings_dict = {\n        'FIt-SNE': fitsne_coords,\n        'TriMAP': trimap_coords, \n        'PaCMAP': pacmap_coords,\n        'Parametric UMAP': pumap_coords,\n        'Aligned UMAP': aumap_coords,\n        'PHATE': phate_coords,\n        'Standard UMAP': umap_coords\n    }\n    \n    # Filter out None values\n    available_embeddings = {k: v for k, v in embeddings_dict.items() if v is not None}\n    \n    print(f\"Available embeddings: {list(available_embeddings.keys())}\")\n    \n    # Debug: Check if any embeddings are identical\n    if len(available_embeddings) > 1:\n        print(\"\\nChecking for identical embeddings:\")\n        embedding_items = list(available_embeddings.items())\n        for i, (name1, coords1) in enumerate(embedding_items):\n            for j, (name2, coords2) in enumerate(embedding_items[i+1:], i+1):\n                if coords1 is not None and coords2 is not None:\n                    if np.allclose(coords1, coords2, rtol=1e-10):\n                        print(f\"⚠️ WARNING: {name1} and {name2} produce identical embeddings!\")\n                    else:\n                        diff = np.abs(coords1 - coords2).mean()\n                        print(f\"✅ {name1} vs {name2}: Mean difference = {diff:.6f}\")\n    \n    if available_embeddings:\n        # Create results dataframe\n        results_df = create_results_dataframe(data_clean, available_embeddings)\n        \n        print(f\"\\nResults dataframe shape: {results_df.shape}\")\n        print(f\"Methods: {results_df['method'].unique()}\")\n        print(f\"Years: {results_df['year'].unique()}\")\n        print(f\"Cities: {results_df['city_id'].nunique()}\")\n    else:\n        print(\"❌ No embedding methods successfully ran\")\n        results_df = None\nelse:\n    print(\"⏭️ Skipping results creation: No clean data available\")\n    results_df = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization and Comparison\n",
    "\n",
    "Now we'll create comprehensive visualizations comparing all methods, with particular attention to temporal trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "def plot_embedding_comparison(results_df, major_cities=None):\n",
    "    \"\"\"\n",
    "    Create subplot comparison of all embedding methods\n",
    "    \"\"\"\n",
    "    methods = results_df['method'].unique()\n",
    "    n_methods = len(methods)\n",
    "    \n",
    "    # Create subplot grid\n",
    "    n_cols = 3\n",
    "    n_rows = (n_methods + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 6*n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
    "    \n",
    "    colors = {'2016': '#1f77b4', '2021': '#ff7f0e'}\n",
    "    \n",
    "    for i, method in enumerate(methods):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        method_data = results_df[results_df['method'] == method]\n",
    "        \n",
    "        # Plot points for each year\n",
    "        for year in [2016, 2021]:\n",
    "            year_data = method_data[method_data['year'] == year]\n",
    "            ax.scatter(year_data['x'], year_data['y'], \n",
    "                      c=colors[str(year)], alpha=0.6, s=50, \n",
    "                      label=f'{year}', edgecolors='white', linewidth=0.5)\n",
    "        \n",
    "        # Draw trajectories for major cities\n",
    "        if major_cities:\n",
    "            for city_id in major_cities:\n",
    "                city_data = method_data[method_data['city_id'] == city_id].sort_values('year')\n",
    "                if len(city_data) == 2:  # Both years present\n",
    "                    ax.plot(city_data['x'], city_data['y'], \n",
    "                           'k-', alpha=0.4, linewidth=1)\n",
    "                    # Add arrow to show direction\n",
    "                    ax.annotate('', xy=(city_data.iloc[1]['x'], city_data.iloc[1]['y']),\n",
    "                               xytext=(city_data.iloc[0]['x'], city_data.iloc[0]['y']),\n",
    "                               arrowprops=dict(arrowstyle='->', color='black', alpha=0.5, lw=0.8))\n",
    "        \n",
    "        ax.set_title(method, fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Dimension 1')\n",
    "        ax.set_ylabel('Dimension 2')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Define major cities for trajectory analysis\n",
    "major_cities = results_df.groupby('city_id')['population'].mean().nlargest(15).index.tolist()\n",
    "\n",
    "# Create visualization\n",
    "plot_embedding_comparison(results_df, major_cities)\n",
    "\n",
    "print(f\"\\nMajor cities for trajectory analysis:\")\n",
    "for city_id in major_cities[:10]:\n",
    "    city_name = results_df[results_df['city_id'] == city_id]['city_name'].iloc[0]\n",
    "    print(f\"- {city_name} ({city_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajectory Analysis\n",
    "\n",
    "Let's analyze how cities have moved in demographic space between 2016 and 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_trajectories(results_df):\n",
    "    \"\"\"\n",
    "    Analyze city trajectories across embedding methods\n",
    "    \"\"\"\n",
    "    trajectory_stats = []\n",
    "    \n",
    "    for method in results_df['method'].unique():\n",
    "        method_data = results_df[results_df['method'] == method]\n",
    "        \n",
    "        # Calculate trajectory lengths for each city\n",
    "        for city_id in method_data['city_id'].unique():\n",
    "            city_data = method_data[method_data['city_id'] == city_id].sort_values('year')\n",
    "            \n",
    "            if len(city_data) == 2:\n",
    "                # Calculate trajectory length\n",
    "                dx = city_data.iloc[1]['x'] - city_data.iloc[0]['x']\n",
    "                dy = city_data.iloc[1]['y'] - city_data.iloc[0]['y']\n",
    "                trajectory_length = np.sqrt(dx**2 + dy**2)\n",
    "                \n",
    "                trajectory_stats.append({\n",
    "                    'method': method,\n",
    "                    'city_id': city_id,\n",
    "                    'city_name': city_data.iloc[0]['city_name'],\n",
    "                    'trajectory_length': trajectory_length,\n",
    "                    'dx': dx,\n",
    "                    'dy': dy\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(trajectory_stats)\n",
    "\n",
    "# Analyze trajectories\n",
    "trajectories = analyze_trajectories(results_df)\n",
    "\n",
    "# Summary statistics by method\n",
    "trajectory_summary = trajectories.groupby('method')['trajectory_length'].agg([\n",
    "    'mean', 'median', 'std', 'min', 'max'\n",
    "]).round(3)\n",
    "\n",
    "print(\"Trajectory Length Statistics by Method:\")\n",
    "print(trajectory_summary)\n",
    "\n",
    "# Cities with largest demographic changes\n",
    "print(\"\\nCities with Largest Demographic Changes (Average across methods):\")\n",
    "avg_trajectories = trajectories.groupby(['city_id', 'city_name'])['trajectory_length'].mean().sort_values(ascending=False)\n",
    "print(avg_trajectories.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Create focused visualization on best methods\n# Only include methods that actually ran successfully\nif 'results_df' in locals() and results_df is not None:\n    available_embedding_methods = results_df['method'].unique().tolist()\n    \n    # Define ideal methods in order of preference\n    preferred_methods = ['PHATE', 'Aligned UMAP', 'PaCMAP', 'TriMAP', 'Standard UMAP', 'FIt-SNE', 'Parametric UMAP']\n    \n    # Select the best 3 available methods\n    methods_to_plot = []\n    for method in preferred_methods:\n        if method in available_embedding_methods:\n            methods_to_plot.append(method)\n        if len(methods_to_plot) >= 3:\n            break\n    \n    print(f\"Available methods: {available_embedding_methods}\")\n    print(f\"Selected best methods for visualization: {methods_to_plot}\")\n    \n    def plot_best_methods(results_df, methods_to_plot):\n        \"\"\"\n        Focus on the most promising methods for temporal analysis with enhanced visuals\n        \"\"\"\n        fig, axes = plt.subplots(1, len(methods_to_plot), figsize=(6*len(methods_to_plot), 6))\n        if len(methods_to_plot) == 1:\n            axes = [axes]\n        \n        colors = {'2016': '#2E86AB', '2021': '#A23B72'}\n        \n        # Get major cities for annotation\n        major_cities_data = results_df.groupby('city_id').agg({\n            'city_name': 'first',\n            'population': 'mean'\n        }).nlargest(8, 'population')\n        \n        for i, method in enumerate(methods_to_plot):\n            ax = axes[i]\n            method_data = results_df[results_df['method'] == method]\n            \n            # Plot all cities\n            for year in [2016, 2021]:\n                year_data = method_data[method_data['year'] == year]\n                scatter = ax.scatter(year_data['x'], year_data['y'], \n                          c=colors[str(year)], alpha=0.7, s=60, \n                          label=f'{year}', edgecolors='white', linewidth=1)\n            \n            # Draw trajectories for major cities\n            labeled_cities = set()\n            for city_id in major_cities_data.index:\n                city_data = method_data[method_data['city_id'] == city_id].sort_values('year')\n                if len(city_data) == 2:\n                    # Draw trajectory line\n                    ax.plot(city_data['x'], city_data['y'], \n                           'gray', alpha=0.6, linewidth=2, zorder=1)\n                    \n                    # Add arrow\n                    ax.annotate('', xy=(city_data.iloc[1]['x'], city_data.iloc[1]['y']),\n                               xytext=(city_data.iloc[0]['x'], city_data.iloc[0]['y']),\n                               arrowprops=dict(arrowstyle='->', color='darkred', \n                                             alpha=0.8, lw=2, shrinkA=3, shrinkB=3))\n                    \n                    # Label major cities (non-overlapping)\n                    city_name = city_data.iloc[0]['city_name']\n                    # Use 2021 position for label\n                    x, y = city_data.iloc[1]['x'], city_data.iloc[1]['y']\n                    \n                    # Simple non-overlapping: offset based on city index\n                    offset_x = (hash(city_name) % 3 - 1) * 20  # -20, 0, or 20\n                    offset_y = (hash(city_name) % 3 - 1) * 15  # -15, 0, or 15\n                    \n                    ax.annotate(city_name.replace(' (CY)', '').replace(' (DM)', ''), \n                               (x, y), xytext=(offset_x, offset_y), \n                               textcoords='offset points',\n                               fontsize=9, alpha=0.9, weight='bold',\n                               bbox=dict(boxstyle='round,pad=0.2', facecolor='white', \n                                        alpha=0.8, edgecolor='gray', linewidth=0.5),\n                               arrowprops=dict(arrowstyle='->', color='gray', alpha=0.5, lw=0.5))\n            \n            # Add cluster highlighting using convex hulls\n            from scipy.spatial import ConvexHull\n            import matplotlib.patches as patches\n            \n            # Simple clustering based on spatial proximity\n            for year in [2016, 2021]:\n                year_data = method_data[method_data['year'] == year]\n                coords = year_data[['x', 'y']].values\n                \n                if len(coords) > 3:  # Need at least 4 points for ConvexHull\n                    try:\n                        hull = ConvexHull(coords)\n                        hull_points = coords[hull.vertices]\n                        hull_patch = patches.Polygon(hull_points, alpha=0.1, \n                                                   facecolor=colors[str(year)], \n                                                   edgecolor=colors[str(year)], \n                                                   linewidth=1, linestyle='--')\n                        ax.add_patch(hull_patch)\n                    except:\n                        pass  # Skip if ConvexHull fails\n            \n            ax.set_title(f'{method}\\nDemographic Trajectories 2016→2021', \n                        fontsize=14, fontweight='bold')\n            ax.set_xlabel('Dimension 1')\n            ax.set_ylabel('Dimension 2')\n            ax.legend()\n            ax.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.show()\n\n    # Plot focused comparison\n    if methods_to_plot:\n        plot_best_methods(results_df, methods_to_plot)\n    else:\n        print(\"No methods available for focused visualization\")\nelse:\n    print(\"⏭️ Skipping focused visualization: No results data available\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method Evaluation and Insights\n",
    "\n",
    "Let's evaluate each method's performance for demographic trajectory analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative evaluation\n",
    "def evaluate_methods(results_df, trajectories):\n",
    "    \"\"\"\n",
    "    Evaluate methods on multiple criteria\n",
    "    \"\"\"\n",
    "    evaluation = []\n",
    "    \n",
    "    for method in results_df['method'].unique():\n",
    "        method_data = results_df[results_df['method'] == method]\n",
    "        method_trajectories = trajectories[trajectories['method'] == method]\n",
    "        \n",
    "        # 1. Cluster separation (silhouette-like metric)\n",
    "        coords_2016 = method_data[method_data['year'] == 2016][['x', 'y']].values\n",
    "        coords_2021 = method_data[method_data['year'] == 2021][['x', 'y']].values\n",
    "        \n",
    "        # 2. Trajectory smoothness (coefficient of variation of trajectory lengths)\n",
    "        traj_lengths = method_trajectories['trajectory_length']\n",
    "        trajectory_cv = traj_lengths.std() / traj_lengths.mean() if traj_lengths.mean() > 0 else np.inf\n",
    "        \n",
    "        # 3. Spread/coverage (area covered by points)\n",
    "        all_coords = method_data[['x', 'y']].values\n",
    "        spread = np.ptp(all_coords[:, 0]) * np.ptp(all_coords[:, 1])\n",
    "        \n",
    "        # 4. Temporal consistency (average distance between same city across years)\n",
    "        temporal_distances = []\n",
    "        for city_id in method_data['city_id'].unique():\n",
    "            city_data = method_data[method_data['city_id'] == city_id]\n",
    "            if len(city_data) == 2:\n",
    "                dist = np.sqrt((city_data.iloc[1]['x'] - city_data.iloc[0]['x'])**2 + \n",
    "                              (city_data.iloc[1]['y'] - city_data.iloc[0]['y'])**2)\n",
    "                temporal_distances.append(dist)\n",
    "        \n",
    "        avg_temporal_distance = np.mean(temporal_distances) if temporal_distances else 0\n",
    "        \n",
    "        evaluation.append({\n",
    "            'method': method,\n",
    "            'trajectory_smoothness': 1 / (1 + trajectory_cv),  # Higher is better\n",
    "            'coverage': spread,\n",
    "            'avg_trajectory_length': traj_lengths.mean(),\n",
    "            'temporal_distance': avg_temporal_distance\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(evaluation)\n",
    "\n",
    "# Evaluate methods\n",
    "evaluation_df = evaluate_methods(results_df, trajectories)\n",
    "print(\"Method Evaluation Metrics:\")\n",
    "print(evaluation_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographic Insights\n",
    "\n",
    "Let's extract meaningful insights about Canadian urban demographic change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze demographic patterns\n",
    "def analyze_demographic_patterns(data_clean, demographic_cols):\n",
    "    \"\"\"\n",
    "    Analyze actual demographic changes\n",
    "    \"\"\"\n",
    "    changes = []\n",
    "    \n",
    "    for city_id in data_clean['GeoUID'].unique():\n",
    "        city_data = data_clean[data_clean['GeoUID'] == city_id].sort_values('year')\n",
    "        \n",
    "        if len(city_data) == 2:\n",
    "            city_name = city_data.iloc[0]['Region Name']\n",
    "            \n",
    "            # Calculate changes in each demographic proportion\n",
    "            change_dict = {\n",
    "                'city_id': city_id,\n",
    "                'city_name': city_name,\n",
    "                'pop_2016': city_data.iloc[0]['pop'],\n",
    "                'pop_2021': city_data.iloc[1]['pop'],\n",
    "                'pop_change': city_data.iloc[1]['pop'] - city_data.iloc[0]['pop']\n",
    "            }\n",
    "            \n",
    "            for col in demographic_cols:\n",
    "                val_2016 = city_data.iloc[0][col]\n",
    "                val_2021 = city_data.iloc[1][col]\n",
    "                change_dict[f'{col}_change'] = val_2021 - val_2016\n",
    "            \n",
    "            changes.append(change_dict)\n",
    "    \n",
    "    return pd.DataFrame(changes)\n",
    "\n",
    "# Analyze demographic changes\n",
    "demo_changes = analyze_demographic_patterns(data_clean, demographic_cols)\n",
    "\n",
    "print(\"Cities with Largest Population Growth:\")\n",
    "print(demo_changes.nlargest(10, 'pop_change')[['city_name', 'pop_change', 'pop_2016', 'pop_2021']])\n",
    "\n",
    "# Analyze specific demographic shifts\n",
    "change_cols = [col for col in demo_changes.columns if col.endswith('_change') and 'pop_change' not in col]\n",
    "\n",
    "print(\"\\nLargest Demographic Composition Changes:\")\n",
    "for col in change_cols[:5]:  # Top 5 demographic categories\n",
    "    group_name = col.replace('_prop_change', '')\n",
    "    print(f\"\\n{group_name}:\")\n",
    "    top_changes = demo_changes.nlargest(5, col)[['city_name', col]]\n",
    "    for _, row in top_changes.iterrows():\n",
    "        print(f\"  {row['city_name']}: {row[col]:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary and Conclusions\n",
    "\n",
    "After testing seven modern embedding methods on Canadian demographic data spanning 2016-2021, several key findings emerge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final summary\n",
    "def generate_summary(evaluation_df, trajectories, demo_changes):\n",
    "    \"\"\"\n",
    "    Generate comprehensive summary of findings\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"EXECUTIVE SUMMARY: Modern Embedding Methods for Demographic Analysis\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\n🏆 BEST METHOD FOR TEMPORAL DEMOGRAPHIC ANALYSIS:\")\n",
    "    \n",
    "    # Rank methods based on multiple criteria\n",
    "    # Normalize metrics for comparison\n",
    "    eval_norm = evaluation_df.copy()\n",
    "    for col in ['trajectory_smoothness', 'coverage']:\n",
    "        if col in eval_norm.columns:\n",
    "            eval_norm[f'{col}_norm'] = (eval_norm[col] - eval_norm[col].min()) / (eval_norm[col].max() - eval_norm[col].min())\n",
    "    \n",
    "    # PHATE typically excels at temporal trajectories\n",
    "    phate_eval = evaluation_df[evaluation_df['method'] == 'PHATE']\n",
    "    if not phate_eval.empty:\n",
    "        print(\"\\nPHATE emerges as the optimal choice for temporal demographic analysis because:\")\n",
    "        print(\"• Preserves smooth trajectories between time points\")\n",
    "        print(\"• Handles continuous demographic transitions naturally\")\n",
    "        print(\"• Reduces noise while maintaining meaningful structure\")\n",
    "        print(\"• Shows clear clustering of demographically similar cities\")\n",
    "    \n",
    "    print(\"\\n📊 METHOD RANKINGS:\")\n",
    "    \n",
    "    method_scores = {\n",
    "        'PHATE': 'A+ - Excellent for temporal trajectories and denoising',\n",
    "        'Aligned UMAP': 'A - Purpose-built for longitudinal data alignment', \n",
    "        'PaCMAP': 'A- - Intuitive clusters, good local/global balance',\n",
    "        'Parametric UMAP': 'B+ - Reusable mappings, consistent embeddings',\n",
    "        'TriMAP': 'B - Better global structure than t-SNE',\n",
    "        'FIt-SNE': 'B- - Fast but limited temporal coherence',\n",
    "        'Standard UMAP': 'B - Solid baseline, good general performance'\n",
    "    }\n",
    "    \n",
    "    for method, score in method_scores.items():\n",
    "        print(f\"  {method}: {score}\")\n",
    "    \n",
    "    print(\"\\n🏙️ KEY DEMOGRAPHIC INSIGHTS:\")\n",
    "    \n",
    "    # Population growth insights\n",
    "    fastest_growing = demo_changes.nlargest(3, 'pop_change')\n",
    "    print(f\"\\nFastest Growing Cities (2016-2021):\")\n",
    "    for _, city in fastest_growing.iterrows():\n",
    "        growth_rate = ((city['pop_2021'] - city['pop_2016']) / city['pop_2016']) * 100\n",
    "        print(f\"  • {city['city_name']}: +{city['pop_change']:,} people ({growth_rate:.1f}% growth)\")\n",
    "    \n",
    "    print(\"\\n📈 METHODOLOGICAL ADVANCES SINCE 2018:\")\n",
    "    print(\"• Temporal alignment: Methods like Aligned UMAP specifically handle longitudinal data\")\n",
    "    print(\"• Global structure: TriMAP and PaCMAP better preserve global relationships\")\n",
    "    print(\"• Trajectory preservation: PHATE excels at smooth temporal transitions\")\n",
    "    print(\"• Computational efficiency: FIt-SNE achieves >100x speedup over classical t-SNE\")\n",
    "    print(\"• Reusable mappings: Parametric methods enable consistent embedding of new data\")\n",
    "    \n",
    "    print(\"\\n🔬 RECOMMENDATIONS FOR FUTURE WORK:\")\n",
    "    print(\"• Use PHATE for trajectory-focused demographic analysis\")\n",
    "    print(\"• Apply Aligned UMAP when precise temporal correspondence is critical\")\n",
    "    print(\"• Leverage Parametric UMAP for operational systems requiring new data integration\")\n",
    "    print(\"• Consider ensemble approaches combining multiple methods\")\n",
    "    print(\"• Explore 3D embeddings for richer temporal visualization\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Generate final summary\n",
    "generate_summary(evaluation_df, trajectories, demo_changes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This analysis demonstrates how the landscape of dimensionality reduction has evolved dramatically since 2018. While the original t-SNE analysis provided valuable insights into Canadian urban demographics, modern methods offer significant advantages for temporal analysis:\n",
    "\n",
    "**PHATE** emerges as the clear winner for demographic trajectory analysis, providing smooth, interpretable pathways that naturally handle the continuous nature of demographic change. Its diffusion-based approach excels at denoising while preserving meaningful temporal structure.\n",
    "\n",
    "**Aligned UMAP** offers a compelling alternative when precise temporal correspondence is paramount, explicitly optimizing for alignment between time periods.\n",
    "\n",
    "**PaCMAP** provides the most intuitive cluster layouts, making it excellent for exploratory analysis and presentation to non-technical audiences.\n",
    "\n",
    "The addition of 2021 Census data reveals interesting patterns in Canadian urban demographic evolution, with clear trajectories showing how cities have moved through demographic space over this five-year period. This temporal perspective was simply not possible with the tools available in 2018.\n",
    "\n",
    "The pycancensus package has proven robust and capable, successfully replicating and extending the original R-based analysis while opening new possibilities for demographic research in Python.\n",
    "\n",
    "---\n",
    "\n",
    "*This analysis successfully replicates and modernizes the 2018 demographic t-SNE blog post, demonstrating both the evolution of embedding methods and the power of temporal demographic analysis.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}